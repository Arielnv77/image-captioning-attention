ğŸŒ ## Image Captioning with Visual Attention

(or: why the model sees â€œa man in a redâ€ everywhere)

ğŸ“Œ Project overview

This project explores Image Captioning, a task that combines Computer Vision and Natural Language Processing to generate textual descriptions from images.

The goal of the project is not to achieve state-of-the-art results, but to:
	â€¢	build a complete and functional pipeline
	â€¢	understand how visual and textual components interact
	â€¢	analyze why and how the model fails under realistic constraints

The final model worksâ€¦ but often poorly â€” and that is precisely the most interesting part of the project.

ğŸ§  Motivation

Image Captioning looks deceptively simple:

â€œJust look at the image and describe it.â€

In practice, it is a highly data-dependent and fragile task.
This project was designed to experience those limitations first-hand by training a model under deliberately constrained conditions:
	â€¢	small dataset
	â€¢	short training
	â€¢	frozen visual encoder
	â€¢	simple recurrent decoder


ğŸ§© Architecture

The model follows a classic Encoderâ€“Decoder with Attention architecture:

ğŸ”¹ Encoder (Vision)
	â€¢	Pretrained ResNet (ImageNet)
	â€¢	Used as a feature extractor
	â€¢	Outputs a spatial feature map (visual regions)
	â€¢	Encoder weights are frozen

ğŸ”¹ Attention mechanism
	â€¢	Computes attention weights over visual regions
	â€¢	Allows the decoder to focus on different parts of the image at each time step

ğŸ”¹ Decoder (Language)
	â€¢	LSTM-based decoder
	â€¢	Generates the caption word by word
	â€¢	Uses teacher forcing during training


ğŸ“‚ Dataset
	â€¢	Based on Flickr8k
	â€¢	A small subset (~800 images) was intentionally used
	â€¢	Each image has multiple human-written captions
	â€¢	This choice highlights:
	â€¢	dataset bias
	â€¢	mode collapse in generation
	â€¢	limitations of language modeling with few examples


âš™ï¸ Training setup
	â€¢	Framework: PyTorch
	â€¢	Loss: Cross-Entropy (padding ignored)
	â€¢	Optimizer: Adam
	â€¢	Training epochs: 3
	â€¢	Hardware: CPU only

Training loss evolution:
        Epoch 1 â†’ Loss: 5.75
        Epoch 2 â†’ Loss: 4.91
        Epoch 3 â†’ Loss: 4.57


Results (honest version)

The model is able to generate captions for unseen images, but the quality is often:
	â€¢	repetitive
	â€¢	overly generic
	â€¢	biased towards frequent patterns

A common example:

Image without people â†’ â€œa man in a man in a redâ€

This behavior is not a bug â€” it is a textbook example of mode collapse caused by:
	â€¢	small dataset size
	â€¢	strong caption frequency bias
	â€¢	limited language modeling capacity


Error analysis (the important part)

Key factors behind the poor captions:
	â€¢	Dataset size: far too small for a generative task
	â€¢	Dataset bias: over-representation of people and common phrases
	â€¢	Frozen encoder: visual features cannot adapt to the task
	â€¢	Simple decoder: limited linguistic expressiveness
	â€¢	Short training: insufficient exposure to diverse patterns

When the model is uncertain, it defaults to the most probable sentence fragments it has seen during training.

This project clearly demonstrates how data limitations dominate architectural choices in generative models.


ğŸ¯ Conclusions

This project shows that:
	â€¢	Image Captioning is significantly harder than it appears
	â€¢	A correct architecture does not guarantee good results
	â€¢	Data quantity and balance are critical
	â€¢	Poor results can still provide strong learning value

Understanding why a model fails is often more valuable than obtaining a superficially good output.
